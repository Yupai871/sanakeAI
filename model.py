import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"æ­£åœ¨ä½¿ç”¨: {device} è¿›è¡Œè®­ç»ƒ")

MODEL_DIR = './model'


def _ensure_dir():
    """ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨"""
    os.makedirs(MODEL_DIR, exist_ok=True)


class Linear_QNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.linear2(x)
        return x

    def save(self, file_name='model.pth'):
        _ensure_dir()
        path = os.path.join(MODEL_DIR, file_name)
        torch.save(self.state_dict(), path)

    def load(self, file_name='model.pth'):
        path = os.path.join(MODEL_DIR, file_name)
        if os.path.exists(path):
            self.load_state_dict(torch.load(path, weights_only=True))
            print(f"ğŸ‰ æˆåŠŸåŠ è½½å‰ä¸–è®°å¿†: {path}")
        else:
            print("ğŸ‘¶ è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„å¤§è„‘ï¼Œå¼€å§‹ä»é›¶å­¦ä¹ ï¼")


class Conv_QNet(nn.Module):
    def __init__(self, in_channels=7, grid_h=24, grid_w=32, output_size=3):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=4, stride=2, padding=1)
        self.conv2 = nn.Conv2d(in_channels=16,          out_channels=32, kernel_size=4, stride=2, padding=1)

        # ä¿®å¤1ï¼šåŠ¨æ€æ¨æ–­ fc1 è¾“å…¥å°ºå¯¸ï¼Œä¸å†ä¾èµ–æ‰‹å·¥è®¡ç®—çš„é­”æ³•æ•°å­— 32*6*8
        # ä¿®æ”¹å·ç§¯å‚æ•°æˆ–æ¸¸æˆåˆ†è¾¨ç‡åï¼Œæ­¤å¤„æ°¸è¿œè‡ªåŠ¨æ­£ç¡®ï¼Œä¸ä¼šäº§ç”Ÿå°ºå¯¸ä¸åŒ¹é…çš„è¿è¡Œæ—¶é”™è¯¯
        with torch.no_grad():
            dummy = torch.zeros(1, in_channels, grid_h, grid_w)
            dummy = F.relu(self.conv1(dummy))
            dummy = F.relu(self.conv2(dummy))
            fc_in = dummy.numel()

        self.fc1 = nn.Linear(fc_in, 256)
        self.fc2 = nn.Linear(256, output_size)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

    def save(self, file_name='model.pth'):
        # ä¿®å¤2ï¼šå¤ç”¨å…¬å…±è¾…åŠ©å‡½æ•°ï¼Œæ¶ˆé™¤ä¸ Linear_QNet çš„é‡å¤ä»£ç 
        _ensure_dir()
        path = os.path.join(MODEL_DIR, file_name)
        torch.save(self.state_dict(), path)

    def load(self, file_name='model.pth'):
        path = os.path.join(MODEL_DIR, file_name)
        if os.path.exists(path):
            # ä¿®å¤3ï¼šè¡¥é½ weights_only=Trueï¼Œä¸ Linear_QNet ä¿æŒä¸€è‡´ï¼Œæ¶ˆé™¤ PyTorch 2.x FutureWarning
            self.load_state_dict(torch.load(path, weights_only=True))
            print(f"ğŸ‰ æˆåŠŸåŠ è½½å‰ä¸–è®°å¿†: {path}")
        else:
            print("ğŸ‘¶ è¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„ CNN å¤§è„‘ï¼Œå¼€å§‹ä»é›¶å­¦ä¹ ï¼")


class QTrainer:
    def __init__(self, model, target_model, lr, gamma):
        self.lr           = lr
        self.gamma        = gamma
        self.model        = model
        self.target_model = target_model
        self.optimizer    = optim.Adam(model.parameters(), lr=self.lr)
        # ä¿®å¤4ï¼šåˆ é™¤æ­»ä»£ç  self.criterion = nn.MSELoss()
        # train_step å®é™…ä½¿ç”¨å¸¦ IS æƒé‡çš„æ‰‹å†™ MSE Lossï¼Œself.criterion ä»æœªè¢«è°ƒç”¨
        self.tau          = 0.005

        # ä¿®å¤5ï¼šæ ¹æ®å®é™…è®¾å¤‡åŠ¨æ€åˆ›å»º GradScaler
        # åŸå§‹ä»£ç ç¡¬ç¼–ç  'cuda'ï¼Œåœ¨ CPU / MPS æœºå™¨ä¸Šç›´æ¥å´©æºƒ
        # enabled=False æ—¶ scaler é€æ˜åœ°é€€åŒ–ä¸ºæ™®é€šåå‘ä¼ æ’­ï¼Œä»£ç è·¯å¾„å®Œå…¨ç»Ÿä¸€
        self._amp_enabled = (device.type == 'cuda')
        self.scaler       = torch.amp.GradScaler(device.type, enabled=self._amp_enabled)

    def train_step(self, state, action, reward, next_state, done, is_weights=None):
        state      = torch.tensor(np.array(state),      dtype=torch.float).to(device)
        next_state = torch.tensor(np.array(next_state), dtype=torch.float).to(device)
        action     = torch.tensor(action,               dtype=torch.long).to(device)
        reward     = torch.tensor(reward,               dtype=torch.float).to(device)
        done       = torch.tensor(done,                 dtype=torch.bool).to(device)

        # å•æ­¥è°ƒç”¨æ—¶è¡¥å…… batch ç»´åº¦
        if len(state.shape) == 3:
            state      = torch.unsqueeze(state,      0)
            next_state = torch.unsqueeze(next_state, 0)
            action     = torch.unsqueeze(action,     0)
            reward     = torch.unsqueeze(reward,     0)
            done       = torch.unsqueeze(done,       0)

        # æ²¡æœ‰ä¼ å…¥æƒé‡ï¼ˆçŸ­æœŸè®°å¿†å•æ­¥è®­ç»ƒï¼‰æ—¶ï¼Œé»˜è®¤æƒé‡å…¨ä¸º 1
        if is_weights is None:
            is_weights = torch.ones(len(done), dtype=torch.float).to(device)
        else:
            is_weights = torch.tensor(is_weights, dtype=torch.float).to(device)

        # ä¿®å¤6ï¼šautocast ä½¿ç”¨åŠ¨æ€è®¾å¤‡ç±»å‹ï¼ŒCPU ä¸Š enabled=False è‡ªåŠ¨è·³è¿‡æ··åˆç²¾åº¦
        with torch.amp.autocast(device_type=device.type, enabled=self._amp_enabled):

            # 1. é¢„æµ‹å½“å‰çŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œçš„ Q å€¼ (shape: Batch Ã— 3)
            pred = self.model(state)

            # 2. Double DQNï¼šonline ç½‘ç»œé€‰åŠ¨ä½œï¼Œtarget ç½‘ç»œä¼°å€¼ï¼Œé˜²æ­¢ Q å€¼è¿‡ä¼°è®¡
            with torch.no_grad():
                next_q_online = self.model(next_state)
                best_actions  = torch.argmax(next_q_online, dim=1)
                next_q_target = self.target_model(next_state)
                max_next_q    = next_q_target.gather(1, best_actions.unsqueeze(1)).squeeze(1)

                # Bellman ç›®æ ‡å€¼ (shape: Batch,)
                Q_new = reward + (~done).float() * self.gamma * max_next_q

            # 3. ç”¨ gather ç²¾å‡†æå–å®é™…æ‰§è¡ŒåŠ¨ä½œå¯¹åº”çš„é¢„æµ‹ Q å€¼
            # one-hot è½¬ç´¢å¼•ï¼š[0, 1, 0] -> 1
            action_indices = torch.argmax(action, dim=1)
            q_acted = pred.gather(1, action_indices.unsqueeze(1)).squeeze(1)

            # 4. è®¡ç®— TD-Errorï¼Œç”¨äºæ›´æ–° PER ä¼˜å…ˆçº§ï¼ˆdetachï¼Œä¸å‚ä¸åå‘ä¼ æ’­ï¼‰
            td_errors = torch.abs(Q_new - q_acted).detach().cpu().numpy()

            # 5. å¸¦é‡è¦æ€§é‡‡æ ·æƒé‡çš„åŠ æƒ MSE Loss
            # Q_new æ— æ¢¯åº¦ï¼Œq_acted ä¿ç•™è®¡ç®—å›¾ï¼Œåå‘ä¼ æ’­æ–¹å‘æ­£ç¡®
            loss = (is_weights * (Q_new - q_acted) ** 2).mean()

        # åå‘ä¼ æ’­ï¼ˆscaler åœ¨ CPU ä¸Š enabled=False æ—¶é€æ˜åœ°ç›´æ¥è°ƒç”¨ backwardï¼‰
        self.optimizer.zero_grad()
        self.scaler.scale(loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()

        # Soft Target Network Update: Î¸_target â† Ï„Â·Î¸_online + (1-Ï„)Â·Î¸_target
        for target_param, local_param in zip(self.target_model.parameters(), self.model.parameters()):
            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

        return loss.item(), td_errors
