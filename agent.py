import torch
import random
import numpy as np
import os
import time
from game import SnakeGameAI, Direction, Point, BLOCK_SIZE
from model import Conv_QNet, QTrainer
from torch.utils.tensorboard import SummaryWriter
from memory import PrioritizedReplayBuffer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MAX_MEMORY = 100000
BATCH_SIZE = 1024
LR         = 0.0003
MODEL_DIR  = './model'


class Agent:
    def __init__(self):
        self.n_games = 0
        self.epsilon = 0
        self.gamma   = 0.9

        self.memory = PrioritizedReplayBuffer(capacity=MAX_MEMORY)

        self.model = Conv_QNet()
        self.model.to(device)

        self.target_model = Conv_QNet()
        self.target_model.to(device)
        self.target_model.load_state_dict(self.model.state_dict())
        self.target_model.eval()

        self.trainer = QTrainer(self.model, self.target_model, lr=LR, gamma=self.gamma)

    # â”€â”€ Checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def save_checkpoint(self, record, filename='checkpoint.pth'):
        os.makedirs(MODEL_DIR, exist_ok=True)
        path = os.path.join(MODEL_DIR, filename)
        checkpoint = {
            'n_games':            self.n_games,
            'record':             record,
            'model_state':        self.model.state_dict(),
            'target_model_state': self.target_model.state_dict(),
            'optimizer_state':    self.trainer.optimizer.state_dict(),
        }
        torch.save(checkpoint, path)

    def load_checkpoint(self, filename='checkpoint.pth'):
        path = os.path.join(MODEL_DIR, filename)
        if os.path.exists(path):
            # ä¿®å¤ï¼šè¡¥é½ weights_only=Trueï¼Œæ¶ˆé™¤ PyTorch 2.x FutureWarning
            checkpoint = torch.load(path, weights_only=True)
            self.n_games = checkpoint['n_games']
            self.model.load_state_dict(checkpoint['model_state'])
            self.target_model.load_state_dict(checkpoint['target_model_state'])
            self.trainer.optimizer.load_state_dict(checkpoint['optimizer_state'])
            print(f"ğŸ‰ æˆåŠŸæ¢å¤è®­ç»ƒç°åœºï¼å½“å‰å±€æ•°: {self.n_games} | å†å²æœ€é«˜åˆ†: {checkpoint['record']}")
            return checkpoint['record']
        else:
            print("ğŸ‘¶ æ²¡æœ‰æ‰¾åˆ°å­˜æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–°çš„å¼€å§‹ï¼")
            return 0

    # â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_state(self, game):
        """
        å°†æ¸¸æˆçŠ¶æ€ç¼–ç ä¸º 4 é€šé“çš„ 24Ã—32 å¼ é‡ï¼š
          é€šé“ 0ï¼šé£Ÿç‰©ä½ç½®ï¼ˆç¨€ç–äºŒå€¼å›¾ï¼‰
          é€šé“ 1ï¼šè›‡å¤´ä½ç½®ï¼ˆç¨€ç–äºŒå€¼å›¾ï¼‰
          é€šé“ 2ï¼šè›‡èº«ä½ç½®ï¼ˆç¨€ç–äºŒå€¼å›¾ï¼‰
          é€šé“ 3ï¼šå½“å‰æ–¹å‘ï¼ˆå…¨å›¾å¡«å……åŒä¸€æµ®ç‚¹å€¼ï¼‰

        æ³¨æ„ï¼šæ–¹å‘æœ¬è´¨ä¸Šæ˜¯ç±»åˆ«å˜é‡ï¼Œç”¨è¿ç»­æ ‡é‡ï¼ˆ0.25/0.5/0.75/1.0ï¼‰å­˜åœ¨
        éšå¼å¤§å°å…³ç³»ã€‚å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯å°†æ­¤é€šé“æ”¹ä¸º 4 ä¸ªç‹¬ç«‹çš„ one-hot
        é€šé“ï¼Œå¹¶åŒæ­¥ä¿®æ”¹ Conv_QNet çš„ in_channels=7ã€‚
        """
        state = np.zeros((4, 24, 32), dtype=np.float32)

        # é€šé“ 0ï¼šé£Ÿç‰©
        fx, fy = int(game.food.x // BLOCK_SIZE), int(game.food.y // BLOCK_SIZE)
        if 0 <= fx < 32 and 0 <= fy < 24:
            state[0, fy, fx] = 1.0

        # é€šé“ 1ï¼šè›‡å¤´
        hx, hy = int(game.head.x // BLOCK_SIZE), int(game.head.y // BLOCK_SIZE)
        if 0 <= hx < 32 and 0 <= hy < 24:
            state[1, hy, hx] = 1.0

        # é€šé“ 2ï¼šè›‡èº«ï¼ˆä¸å«å¤´éƒ¨ï¼‰
        for pt in game.snake[1:]:
            bx, by = int(pt.x // BLOCK_SIZE), int(pt.y // BLOCK_SIZE)
            if 0 <= bx < 32 and 0 <= by < 24:
                state[2, by, bx] = 1.0

        # é€šé“ 3ï¼šå½“å‰æ–¹å‘ï¼ˆé“ºæ»¡å…¨å›¾ï¼‰
        dir_map = {
            Direction.UP:    0.25,
            Direction.RIGHT: 0.50,
            Direction.DOWN:  0.75,
            Direction.LEFT:  1.00,
        }
        state[3, :, :] = dir_map.get(game.direction, 0.0)

        return state

    # â”€â”€ Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def remember(self, state, action, reward, next_state, done):
        self.memory.add((state, action, reward, next_state, done))

    # â”€â”€ Training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def train_long_memory(self):
        """ä» PER æ± æ‰¹é‡é‡‡æ ·å¹¶è®­ç»ƒï¼ŒåŒæ­¥æ›´æ–°ç»éªŒä¼˜å…ˆçº§"""
        if len(self.memory) < BATCH_SIZE:
            return 0.0

        mini_sample, tree_idxs, is_weights = self.memory.sample(BATCH_SIZE)
        states, actions, rewards, next_states, dones = zip(*mini_sample)

        loss, td_errors = self.trainer.train_step(
            states, actions, rewards, next_states, dones, is_weights=is_weights
        )
        self.memory.update_priorities(tree_idxs, td_errors)
        return loss

    def train_short_memory(self, state, action, reward, next_state, done):
        """å•æ­¥åœ¨çº¿è®­ç»ƒï¼ˆçŸ­æœŸè®°å¿†ï¼‰ï¼Œä¸ä½¿ç”¨ IS æƒé‡ï¼Œä¸æ›´æ–°ç»éªŒæ± ä¼˜å…ˆçº§"""
        loss, _ = self.trainer.train_step(state, action, reward, next_state, done)
        return loss

    # â”€â”€ Action â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def get_action(self, state):
        """
        Îµ-greedy æ¢ç´¢ç­–ç•¥ï¼š
          å‰ 5000 å±€çº¿æ€§è¡°å‡ï¼ˆ100% -> 1%ï¼‰ï¼Œä¹‹åé”å®šåœ¨ 1% ç»´æŒå°‘é‡éšæœºæ¢ç´¢ã€‚

        ä¿®å¤ï¼šåŸä»£ç ç”¨ random.randint(0, 100) ç”Ÿæˆ 0~100 å…± 101 ä¸ªæ•´æ•°ï¼Œ
        å¯¼è‡´æ¦‚ç‡è®¡ç®—åå·®çº¦ 1%ã€‚æ”¹ä¸º random.random() ç”Ÿæˆ [0,1) å‡åŒ€æµ®ç‚¹æ•°ï¼Œ
        ä¸ epsilon/100 æ¯”è¾ƒï¼Œç¡®ä¿æ¦‚ç‡ç²¾ç¡®ã€‚
        """
        self.epsilon = max(1, 100 - (self.n_games / 50))

        final_move = [0, 0, 0]
        if random.random() * 100 < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºåŠ¨ä½œ
            move = random.randint(0, 2)
            final_move[move] = 1
        else:
            # åˆ©ç”¨ï¼šæ¨¡å‹é¢„æµ‹æœ€ä¼˜åŠ¨ä½œ
            state0 = torch.tensor(state, dtype=torch.float).unsqueeze(0).to(device)
            with torch.no_grad():
                prediction = self.model(state0)
            move = torch.argmax(prediction).item()
            final_move[move] = 1

        return final_move


# â”€â”€ Train Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train():
    agent = Agent()
    game  = SnakeGameAI()

    record = agent.load_checkpoint('checkpoint.pth')

    run_name = f"Snake_CNN_PER_{int(time.time())}"
    writer   = SummaryWriter(f'logs/{run_name}')
    print(f"ğŸ“ˆ å®æ—¶ç›‘æ§å·²å¼€å¯ï¼Œè¯·åœ¨ç»ˆç«¯è¾“å…¥: tensorboard --logdir=logs")

    current_score = 0

    try:
        while True:
            state_old  = agent.get_state(game)
            final_move = agent.get_action(state_old)

            # æ¯ 50 å±€æˆ–é«˜åˆ†æ—¶å¼€å¯æ¸²æŸ“ï¼Œå…¶ä½™æ—¶é—´å…¨é€Ÿè®­ç»ƒ
            show_screen = (agent.n_games % 50 == 0) or (current_score > 30)
            reward, done, score = game.play_step(final_move, render=show_screen)
            current_score = score

            state_new  = agent.get_state(game)

            # çŸ­æœŸè®°å¿†ï¼šæ¯æ­¥åœ¨çº¿æ›´æ–°ä¸€æ¬¡
            agent.train_short_memory(state_old, final_move, reward, state_new, done)
            agent.remember(state_old, final_move, reward, state_new, done)

            if done:
                game.reset()
                agent.n_games += 1

                # é•¿æœŸè®°å¿†ï¼šæ¯å±€ç»“æŸåæ‰¹é‡è®­ç»ƒ
                long_loss = agent.train_long_memory()

                if score > record:
                    record = score
                    agent.save_checkpoint(record, 'best_model.pth')
                    print("ğŸ† ç ´çºªå½•äº†ï¼å·²å•ç‹¬ä¿å­˜ Best Modelã€‚")

                if agent.n_games % 50 == 0:
                    agent.save_checkpoint(record, 'checkpoint.pth')

                print(
                    f'Game {agent.n_games:>5} | '
                    f'Epsilon: {agent.epsilon:5.1f}% | '
                    f'Score: {score:>3} | '
                    f'Record: {record:>3} | '
                    f'Loss: {long_loss:.6f}'
                )

                writer.add_scalar('Loss/Training',    long_loss,        agent.n_games)
                writer.add_scalar('Score/Current',    score,            agent.n_games)
                writer.add_scalar('Score/Record',     record,           agent.n_games)
                writer.add_scalar('Metrics/Epsilon',  agent.epsilon,    agent.n_games)

                current_score = 0

    except KeyboardInterrupt:
        print("\nğŸ›‘ æ¥æ”¶åˆ°æ‰‹åŠ¨åœæ­¢ä¿¡å· (Ctrl+C)ï¼Œæ­£åœ¨ä¿å­˜æœ€åçš„è®­ç»ƒç°åœº...")
        agent.save_checkpoint(record, 'checkpoint.pth')
        print("âœ… å­˜æ¡£æˆåŠŸï¼ä½ å¯ä»¥å®‰å…¨å…³é—­äº†ï¼Œä¸‹æ¬¡è¿è¡Œä¼šè‡ªåŠ¨æ¥ä¸Šè¿›åº¦ã€‚")
    finally:
        writer.close()


if __name__ == '__main__':
    train()
